\chapter{Discussion}

The basic idea of the project was to apply control theory, normally used in automation and robotics, to a L-PBF system. Control theory is a wide field of research containing many methods so only some of the insights could be brought into this project. Examples of methods that weren't applied are: dynamic modelling, vision based control and stability assessment \cite{robot-control}. When the project started, the L-PBF system had no capability for closed-loop control. Closed-loop is a prerequisite for a lot of the methods of control theory, and given the limited time, it was decided in this project to focus on the areas that could be applied without closed-loop control and thereby be immediately implemented.

\section{Kinematic model}

Making the kinematic model in the beginning was helpful to get an understanding of the system. But it turned out that it was the f-theta relation that mostly dictated the relation between input and output of the system. So a more direct approach could have been taken, focusing on the f-theta lens. How the actual system performance deviated from the f-theta relation could not have been explained without considering the rest of the system though. But a kinematic model was not directly needed for this, so taking this more direct approach would have been more efficient.

A different mathematical approach could also have been taken in making the kinematic model. Instead of using vector equations that always included all three coordinates, a more coordinate-by-coordinate approach would have given other insights. Many of the operations only depended on one of the coordinates, and pointing this out could have better shown how the different pieces related geometrically. The vector approach was chosen because it worked better for use in the computer algebra system (CAS) programming.

\subsection{Further work for modelling}
\begin{itemize}
    \item The laser beam width could be included in the model instead of representing it by a single line. This step is almost immediately applicable by using a disc to represent the laser source instead of a single point.
    \item The accuracy of the f-theta lens model is the main bottleneck in predicting the behaviour of the system, so there is good reason to improve this. This could be done by either finding a way to integrate the black box models from the manufacturers, or by extending the simple idealised model of the f-theta lens with more theory or experimental data.
    \item The natural next step from the kinematic model is a dynamic model. This is the basis of all closed-loop control.
\end{itemize}

\section{Discussion of calibration} \label{sec:calibration-discussion}

The results of the calibration experiment were in general satisfactory. But while the accuracy of the x-coordinate was greatly improved, the calibration of the y-coordinate only improved a little. There could be multiple reasons for this. Obviously there is always the statistical uncertainty. The reason might have been random errors that coincided in a particular way, even though the probability for this was low. Carrying out the calibration procedure multiple times and on different machines would help in investigating this. Another reason could be the fact that only three simple terms were included in the model. So there can be systematic errors that followed more complex behaviours that weren't calibrated for

A different method that has been proposed for calibration of L-PBF systems is to use a camera, to do in-situ calibration by comparing different laser positions with a reference \cite{in-situ}. This would have the advantage that the whole calibration procedure could be done in one step. A shorter calibration procedure would make it easier to calibrate more often, which is relevant because the system performance can drift over time. However, the computer vision technology for this kind of automated reference comparison would require more work than was possible within the scope of this project.

In the data analysis for the calibration there was the observation that some polynomial terms could not be used to estimate the measured values because their sign was different which led to cancellation of correlation estimates. This cancellation could have been avoided by using the absolute value function at the right places in the data analysis and implementation. While this might improve overall accuracy it would also give a non smooth function at x=0 or y=0, which could result in making surfaces that are supposed to be flat have a slight nick. Besides, there was no clear indication that any specific of the problematic terms was necessary for the calibration, so it was decided to leave them out.

\subsection{Further work for calibration}

\begin{itemize}
    \item The spark marks were one of the main bottlenecks to the precision of the calibration. By adjusting the airflow, that is already implemented a small distance over the build plate, to flow closer to the build plate, these sparks might be avoided.
    \item The ex-situ approach used in this project, where the calibration medium had to be moved from the build chamber to separate measurement equipment could be improved by designing a way to fixate the calibration medium identically in the build chamber and measurement instrument. This would reduce measurement errors coming from the alignment of the coordinate system and thus let the real scanner movement errors stand out more visibly.
    \item While it was not possible to analytically to work with third order terms in the calibration calculations, it might be possible numerically. Finding a way to numerically solve for the nominal values given a function definition could possibly give great results since the third order terms seemed to improved the fit greatly and agreed with usual error characteristics.
\end{itemize}

\section{Discussion of trajectory planning} \label{sec:trajectory-discussion}

Originally the laser moved from place to place in straight lines. A straight line is in fact just a first order polynomial trajectory, so using third or fifth order polynomial trajectories can be seen as a generalisation of the baseline method. Both the modified cubic trajectory and the quintic trajectory have some clear advantages. They both do a good job at ensuring that the mirrors have the right velocity and acceleration in the beginning of the scan trajectories, and thus implement the so-called sky riding feature. On the other hand they are more unpredictable in terms of duration and path length.

In the modified cubic approach, a maximum speed was used to determine the total time of the trajectory. It could be considered to adapt the time formula to respect a given maximum acceleration instead of the speed. In an analogue scanner system, this might be sensible since the acceleration would be proportional to the power needed to move the mirrors. In the digital implementation used in Baxter this is not really sensible though. The trajectories are divided into line segments of constant speed. So during these line segments the acceleration is zero and in between it is infinite (undefined if you must). So nothing is really won by a (digital) limit on acceleration.

The whole motivation for planning the trajectories and get constant scanning speed is that the quality of the consolidated material depends on the energy density during the consolidation. But there is another value that influences the energy density than the velocity, namely the laser power. So a different way to achieve constant energy density on the scan path could be to close loop-control the laser power based on the velocity. If this is done well enough, it's not strictly necessary to keep constant velocity throughout the melt path. But the capacity for that kind of closed-loop control is unproven in our system as of now, and even if it was, it is sane to both ensure good starting conditions and regulate on the way.

\subsection{Further work for trajectory planning}
\begin{itemize}
    \item Bounds for the time and path length of the different scenarios could be calculated based on analysis of the time estimation formulas and the speed limits. This would help to get a picture of how much time the trajectories add (or save) to the manufacturing process and thus in which scenarios they are an advantage or not. It would also be illuminating to investigate it experimentally, by carrying out an experiment used for the verification in this project and measure the elapsed time.
    \item When verifying the trajectories, the GLAMS stopped the mirrors regularly which interrupted the trajectories and thus negated the intended effect of smoothly changing direction and velocity. If these interruptions are removed it would give important insight to do the verification more quantitatively with clear markings of the beginning and end of the melt trajectories and use a faster frequency of the laser. That way it could be verified not only if the move trajectories work as intended but also if they have the intended effect on the melt trajectories.
    \item The use case of the polynomial trajectories can be extended to not only work when moving the laser from one position to another, but also help when a sharp corner is needed in a contour. Sharp bends are difficult because the inertia of the mirrors prevent them from switching direction instantaneously, and stopping causes too much energy to be dissipated in one place. Using a trajectory in between the two lines meeting at the corner could help to reorient the laser before doing the second line. This would require an update to the time determining function, because it can't handle trajectories with identical start- and end-position as it is.
\end{itemize}